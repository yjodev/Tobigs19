{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c833df3",
   "metadata": {},
   "source": [
    "# VAE v.s. GAN\n",
    "\n",
    "## VAE\n",
    "- Variational Auto-Encoder  \n",
    "\n",
    "- 복잡한 데이터 생성 모델을 설계하고 대규모 set에 적응 할 수 있게 해줌\n",
    "\n",
    "- input data를 잠재변수 z로 encoding 한 후, 스스로 input을 복원해 내는 방법\n",
    "\n",
    "- VAE에서의 Loss functoion은 input x와 복원된 x'(decoding 된 x) 간의 Loss로 정의\n",
    "\n",
    "- VAE에서는 Auto-Encoder가 input을 따라 그리는 것에만 맞게 학습 시킴\n",
    "\n",
    "- 결론적으로 z는 의미론적이지 않음\n",
    "\n",
    "## GAN\n",
    "- Generative Adversarial Nets\n",
    "\n",
    "- Generator과 Discriminator가 서로 대립하여 서로의 성능을 점차 개선해 나가는 개념\n",
    "\n",
    "- ex) 지폐위조범(Generator)은 경찰(Discriminator)을 최대한 열심히 속이기 위해 노력함\n",
    "\n",
    "   경찰은 지폐위조범의 위조된 지폐를 감별하기 위해(Classify) 노력함\n",
    "\n",
    "   이런 경쟁 속에서 두 그룹 모두 속이는 능력, 구별하는 능력이 발전하게 됨\n",
    "\n",
    "     => 결과적으로, 진짜 지폐와 위조 지폐를 구별할 수 없을 정도에 이름\n",
    "\n",
    " \n",
    "\n",
    "- Generative model G -> data x의 distibution을 알아내려고 노력\n",
    "\n",
    "  (G가 data distribution을 모사할 수 있으면 sample과 data를 구별할 수 있다.)\n",
    "\n",
    " \n",
    "\n",
    "- Discriminator model D -> sample이 training data인지, G가 만들어낸 data 인지 구별하여 각각의 확률을 estimate시킴\n",
    "\n",
    "\n",
    "\n",
    "## SUMMARY\n",
    "\n",
    "1. GAN\n",
    "- generator model의 목적 자체가 어떤 data의 분포를 학습하는 것이 아님\n",
    "\n",
    "- 진짜 같은 sample을 generate하는 것이 목적\n",
    "\n",
    " \n",
    "2. VAE\n",
    "\n",
    "- data의 분포를 학습하고 싶은데, 이 data가 다루기 힘들기 때문에 variational inference(변화 추론)하는 방법\n",
    "\n",
    "- VAE는 data 분포가 잘 학습되기만 하면 sampling (=data generation)이 저절로 따라옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a75be1",
   "metadata": {},
   "source": [
    "## Bayesian Probability\n",
    "\n",
    "세상에 반복할 수 없는 혹은 알 수 없는 확률들, 즉 일어나지 않은 일에 대한 확률을 사건과 관련이 있는 여러 확률들을 이용해 우리가 알고싶은 사건을 추정하는 것이 베이지안 확률이다.\n",
    "- latent : ‘잠재하는’, ‘숨어있는’, ‘hidden’의 뜻을 가진 단어. 여기서 말하는 latent variable z는 특징(feature)를 가진 vector로 이해하면 좋다.\n",
    "- intractable : 문제를 해결하기 위해 필요한 시간이 문제의 크기에 따라 지수적으로 (exponential) 증가한다면 그 문제는 난해 (intractable) 하다고 한다.\n",
    "- explicit density model : 샘플링 모델의 구조(분포)를 명확히 정의\n",
    "- implicit density model : 샘플링 모델의 구조(분포)를 explicit하게 정의하지 않음\n",
    "- density estimation : x라는 데이터만 관찰할 수 있을 때, 관찰할 수 없는 x가 샘플된 확률밀도함수(probability density function)을 estimate하는 것\n",
    "- Gaussian distribution : 정규분포\n",
    "- Bernoulli distribution : 베르누이분포\n",
    "- Marginal Probability : 주변 확률 분포\n",
    "- D_kl : 쿨백-라이블러 발산(Kullback–Leibler divergence, KLD), 두 확률분포의 차이\n",
    "- Encode / Decode: 암호화,부호화 / 암호화해제,부호화해제\n",
    "- likelihood : 가능도."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5c14f",
   "metadata": {},
   "source": [
    "## VAE GOAL\n",
    "\n",
    "- How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets?\n",
    "\n",
    "VAE의 목표는 Generative Model의 목표와 같다. \n",
    "(1) data와 같은 분포를 가지는 sample 분포에서 sample을 뽑고\n",
    "(2) 어떤 새로운 것을 생성해내는 것이 목표다. 즉,\n",
    "\n",
    "    (1) 주어진 training data가 p_data(x)(확률밀도함수)가 어떤 분포를 가지고 있다면, sample 모델 p_model(x) 역시 같은 분포를 가지면서, (sampling 부분)\n",
    "    (2) 그 모델을 통해 나온 inference 값이 새로운 x라는 데이터이길 바란다. (Generation 부분)\n",
    "    \n",
    "    예를 들어, 몇 개의 다이아몬드(training data)를 가지고 있다고 생각해보자. 그러면 training 다이아몬드 뿐만아니라 모든 다이아몬드의 확률분포와 똑같은 분포를 가진 모델에서 값을 뽑아(1. sampling) training 시켰던 다이아몬드와는 다른 또 다른 다이아몬드(new)를 만드는(generate) 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d5350",
   "metadata": {},
   "source": [
    "## Structure of VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de902381",
   "metadata": {},
   "source": [
    "input 그림이 있을 때 어떤 의미를 가진 구조를 거쳐 output이 나오게 되는지 3 단계로 나누어 살펴보자.\n",
    "\n",
    "1. input: x –> 𝑞_∅ (𝑥)–> 𝜇_𝑖,𝜎_𝑖\n",
    "2. 𝜇_𝑖, 𝜎_𝑖, 𝜖_𝑖 –> 𝑧_𝑖\n",
    "3. 𝑧_𝑖 –> 𝑔_𝜃 (𝑧_𝑖) –> 𝑝_𝑖 : output\n",
    "\n",
    "## 1. Encoder\n",
    "input: x –> 𝑞_∅ (𝑥)–> 𝜇_𝑖,𝜎_𝑖\n",
    "Input shape(x) : (28,28,1)\n",
    "𝑞_∅ (𝑥) 는 encoder 함수인데, x가 주어졌을때(given) z값의 분포의 평균과 분산을 아웃풋으로 내는 함수이다.\n",
    "다시말해 q 함수(=Encoder)의 output은 𝜇_𝑖,𝜎_𝑖 이다.\n",
    "어떤 X라는 입력을 넣어 인코더의 아웃풋은 𝜇_𝑖,𝜎_𝑖 이다. 어떤 데이터의 특징을(latent variable) X를 통해 추측한다. 기본적으로 여기서 나온 특징들의 분포는 정규분포를 따른다고 가정한다. 이런 특징들이 가지는 확률 분포 𝑞_∅ (𝑥) (정확히 말하면 $의 true 분포 (= $)를 정규분포(=Gaussian)라 가정한다는 말이다. 따라서 latent space의 latent variable 값들은 𝑞_∅ (𝑥)의 true 분포를 approximate하는 𝜇_𝑖,𝜎_𝑖를 나타낸다.\n",
    "\n",
    "Encoder 함수의 output은 latent variable의 분포의 𝜇 와 𝜎 를 내고, 이 output값을 표현하는 확률밀도함수를 생각해볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e0341",
   "metadata": {},
   "source": [
    "## 2. Reparameterization Trick (Sampling)\n",
    "𝜇_𝑖, 𝜎_𝑖, 𝜖_𝑖 –> 𝑧_𝑖\n",
    "\n",
    "만약 Encoder 결과에서 나온 값을 활용해 decoding 하는데 sampling 하지 않는다면 어떤 일이 벌어질까? 당연히 는 한 값을 가지므로 그에 대한 decoder(NN)역시 한 값만 뱉는다. 그렇게 된다면 어떤 한 variable은 무조건 똑같은 한 값의 output을 가지게 된다.\n",
    "\n",
    "하지만 Generative Model, VAE가 하고 싶은 것은, 어떤 data의 true 분포가 있으면 그 분포에서 하나를 뽑아 기존 DB에 있지 않은 새로운 data를 생성하고 싶다. 따라서 우리는 필연적으로 그 데이터의 확률분포와 같은 분포에서 하나를 뽑는 sampling을 해야한다. 하지만 그냥 sampling 한다면 sampling 한 값들을 backpropagation 할 수 없다.(아래의 그림을 보면 직관적으로 이해할 수 있다) 이를 해결하기 위해 reparmeterization trick을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c8e41",
   "metadata": {},
   "source": [
    "### Reparmeterization Trick\n",
    "reparameterization trick은 VAE에서 학습 시 그래디언트 역전파(backpropagation)를 가능하게 하는 기술입니다.\n",
    "\n",
    "VAE에서 모델은 잠재 변수 분포를 모델링하고 이를 기반으로 데이터 샘플을 생성함으로써 실제 데이터 분포를 근사합니다. 잠재 변수 분포는 일반적으로 평균과 분산 매개변수를 예측하는 인코더 신경망을 사용하여 정의됩니다. 학습 과정에서 목표는 실제 데이터 분포와 생성된 분포 간의 차이를 최소화하는 것입니다.\n",
    "\n",
    "기존의 역전파 알고리즘의 문제점은 그래디언트가 모델의 매개변수에 대해 계산되어야 하는데, 이 매개변수는 잠재 변수 분포의 평균과 분산입니다. 하지만 잠재 변수 분포에서 샘플을 추출하는 것은 확률적(stochastic) 연산이기 때문에 그래디언트를 계산할 수 없습니다.\n",
    "\n",
    "reparameterization trick은 보조 변수 ε ~ N(0,1)를 도입하여 잠재 변수 z를 z = μ + σε로 재정의함으로써 이 문제를 극복합니다. 이 재정의는 확률적 연산을 모델 외부로 이동시키므로 그래디언트를 이 연산을 통해 계산할 수 있게 합니다.\n",
    "\n",
    "이를 통해 VAE는 표준적인 역전파 기술을 사용하여 학습할 수 있으며, 모델이 실제 데이터 분포를 효과적으로 근사하도록 매개변수를 최적화할 수 있게 됩니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8153538",
   "metadata": {},
   "source": [
    "## 3. Decoder\n",
    "\n",
    "𝑧_𝑖 –> 𝑔_𝜃 (𝑧_𝑖) –> 𝑝_𝑖 : output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48cb11",
   "metadata": {},
   "source": [
    "z 값을 g 함수(decoder)에 넣고 deconv(코드에서는 Conv2DTranspose)를 해 원래 이미지 사이즈의 아웃풋 z_decoded가 나오게 된다. \n",
    "이때 p_data(x)의 분포를 Bernoulli 로 가정했으므로(이미지 recognition 에서 Gaussian 으로 가정할때보다 Bernoulli로 가정해야 의미상 그리고 결과상 더 적절했기 때문) output 값은 0~1 사이 값을 가져야하고, 이를 위해 activatino function을 sigmoid로 설정해주었다. \n",
    "(Gaussian 분포를 따른다고 가정하고 푼다면 아래 loss를 다르게 설정해야한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b4087",
   "metadata": {},
   "source": [
    "## VAE 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11a701",
   "metadata": {},
   "source": [
    "### Loss Fucntion \n",
    "Variational Autoencoder(VAE)의 손실 함수는 재구성 손실(reconstruction loss)과 정규화 손실(regularization loss) 두 가지로 구성됩니다.\n",
    "\n",
    "1. 재구성 손실은 디코더 네트워크에 의해 생성된 출력과 입력 데이터 간의 차이를 측정합니다. 목표는 이 손실을 최소화하여 생성된 출력이 입력 데이터와 유사하도록하는 것입니다. 재구성 손실은 일반적으로 잠재 변수를 고려한 데이터의 음의 로그 우도 또는 입력과 출력 간의 평균 제곱 오차로 정의됩니다.\n",
    "\n",
    "2. 정규화 손실은 잠재 변수 분포가 일반적으로 표준 정규 분포와 유사하도록 권장합니다. 이는 사전 분포와 잠재 변수 분포 사이의 Kullback-Leibler(KL) 발산을 최소화하여 달성됩니다. KL 발산을 최소화하면 잠재 변수가 사전 분포에 따라 분포되도록 보장하며, 모델이 학습 데이터에 과적합되는 것을 방지합니다.\n",
    "\n",
    "- 따라서 VAE의 전체 손실 함수는 재구성 손실과 정규화 손실의 합으로 표현됩니다. 목표는 이 손실을 학습 중에 최소화하여 모델의 매개 변수를 최적화하여 실제 데이터 분포와 유사한 데이터를 생성하는 것입니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3252468",
   "metadata": {},
   "source": [
    "## Conditional VAE\n",
    "\n",
    "- Condition\n",
    "    - label 정보를 알고 있으면 Encoder와 Decoder에서 사용하라\n",
    "    - 구현 시 label의 정보를 추가해주면 된다\n",
    "\n",
    "위의 condition을 제공해주는 것이 바로 Conditional VAE입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5e74c",
   "metadata": {},
   "source": [
    "## CVAE v.s. Vanilla VAE\n",
    "\n",
    "### 개념 비교\n",
    "\n",
    "앞서 살펴본 VAE는 latent space가 임의로 sampling되면 VAE는 어떤 숫자가 샘플링될지 제어할 수 없다. \n",
    "\n",
    "하지만 CVAE는 생성할 숫자의 조건(one-hot lable)을 도입함으로써 이 문제를 해결할 수 있다. \n",
    "\n",
    "이 조건은 Encoder와 Decoder에 모두 제공된다 .\n",
    "\n",
    "\n",
    "### 계산 과정 비교\n",
    "y라는 노드가 새롭게 추가된다. \n",
    "Conditional VAE에서는 y(label 정보)를 알고 있다면 Encoder, Decoder에 추가하라고 했습니다. \n",
    "그런 조건(Condition)이 Computational Graph에 표현되어 있다.\n",
    "\n",
    "Encoder 단에서는 latent vector z를 찾기 위해 x와 함께 y값을 given으로 주었습니다. → qΦ(z|x, y)\n",
    "\n",
    "마찬가지 Decoder 단에서는 data를 생성하는 과정에서  y값을 같이 넣어줬습니다. → p𝜃(x|z, y)\n",
    "\n",
    "또한 M2(CVAE)의 경우 최적화를 했을 때 최적화 식이 ELBO와 같게 됩니다\n",
    "\n",
    "### 성능 비교\n",
    "\n",
    "CVAE의 성능이 더 좋다. 이미 숫자의 정보(label 정보)를 알고 있기 때문.\n",
    "\n",
    "label 정보를 모를 때는 어떻게 학습할 수 있을까?\n",
    "\n",
    "1. 기존의 Vanilla VAE를 사용하거나\n",
    "\n",
    "2. y(label 정보)를 추정해서 학습할 수 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa16a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
